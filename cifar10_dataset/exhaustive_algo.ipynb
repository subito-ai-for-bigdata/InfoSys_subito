{"cells":[{"cell_type":"markdown","metadata":{"id":"1GzTKpkhdMjS"},"source":["# Install and Import all the necessary packages and libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YHT9G1A_F1sn","executionInfo":{"status":"ok","timestamp":1763033540427,"user_tz":-120,"elapsed":9961,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import fashion_mnist, mnist, cifar10, cifar100\n","import numpy as np\n","import time\n","import math\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.python.keras import backend as K\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import gc\n","import multiprocessing\n","from multiprocessing import Process, Queue"]},{"cell_type":"markdown","metadata":{"id":"ShAbrioMXnZP"},"source":["# Definition of programmer's parameters (Controller's block)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7114,"status":"ok","timestamp":1763033547538,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"},"user_tz":-120},"id":"WuM2BFnaHArA","outputId":"489b8786-26d4-4a63-8cf3-b32a9ca9b395"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"]}],"source":["#@markdown ##Dataset\n","topic_prefix = \"cifar10\" # @param [\"cifar10\", \"cifar100\", \"mnist\", \"fashion_mnist\"] {type:\"string\"}\n","if topic_prefix == \"cifar10\":\n","  dataset = cifar10.load_data()\n","elif topic_prefix == \"cifar100\":\n","  dataset = cifar100.load_data()\n","elif topic_prefix == \"mnist\":\n","  dataset = mnist.load_data()\n","elif topic_prefix == \"fashion_mnist\":\n","  dataset = fashion_mnist.load_data()\n","\n","#@markdown ##Synopses-based Training Optimization Configuration\n","# Percentage of the dataset that will be used for training\n","sample_size_low = 0.8 #@param {type:\"slider\", min:0, max:0.99, step:0.05}\n","sample_size_step = 0.15 #@param {type:\"number\"}\n","sample_size_high = 1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n","\n","# Percentage of the dataset that will be used for testing\n","perc_test = 1 #@param {type:\"number\"}\n","\n","# The ID of the method that will be employed during sampling\n","# 0: Simple reservoir sampling in our initial training dataset\n","# 1: Reservoir sampling in each class based on the number of samples (per class)\n","sampling_method_id = 1 #@param {type:\"integer\"}\n","\n","#@markdown ##NN Architecture Configuration\n","\n","total_num_of_layers = 5 #@param {type:\"integer\"}\n","\n","set_of_layers = ['conv', 'pool', 'dense']\n","\n","# Number of epochs for the training process\n","# One Epoch is when an ENTIRE (training) dataset is passed forward and backward through the neural network only once.\n","# NOTE: An epoch is comprised of one or more batches.\n","num_of_epochs_low = 1 #@param {type:\"slider\", min:1, max:30, step:1}\n","num_of_epochs_step = 4 #@param {type:\"integer\"}\n","num_of_epochs_high = 10 #@param {type:\"slider\", min:1, max:30, step:1}\n","\n","# The value for the learning rate for the training process (it is inserted to the Adam optimizer)\n","# During the optimization, the algorithm needs to take a series of tiny steps to descend the error mountain in order to minimize the error.\n","# The direction of the step is determined by the gradient, while the step size is determined by the learning rate.\n","lr_low = 1e-3 #@param {type:\"number\"}\n","lr_high = 1e-3 #@param {type:\"number\"}\n","lr_list = np.geomspace(lr_low, lr_high, num = int(np.log10(lr_high) - np.log10(lr_low)) + 1).tolist()\n","\n","# The size of the batch for the training process\n","# Total number of training examples present in a single batch.\n","# or, number of samples processed before the model is updated.\n","size_of_batch_low = 64 #@param {type:\"integer\"}\n","size_of_batch_step = 64 #@param {type:\"integer\"}\n","size_of_batch_high = 64 #@param {type:\"integer\"}\n","\n","#@markdown ##Tradeoff score Configuration\n","# Maximum training time above which we cancel the experiment (in seconds)\n","# It is used in the score (accuracy--speed tradeoff) formula\n","theta_parameter = 10 #@param {type:\"number\"}\n","\n","# Weight for the accuracy of the model. Max value: 0.99\n","# There is also the weight of training speed of the model (1 - lamda_acc)\n","# It is used in the score (accuracy--speed tradeoff) formula\n","lamda_acc = 0.5 #@param {type:\"number\"}\n","\n","\n","#@markdown ##Other Configuration(s)\n","\n","CONV_NEURONS_CONST = 32 #@param {type:\"integer\"}\n","CONV_NEURONS_BOUND = 256 #@param {type:\"integer\"}\n","DENSE_NEURONS_CONST = 128 #@param {type:\"integer\"}\n","DENSE_NEURONS_BOUND = 32 #@param {type:\"integer\"}\n"]},{"cell_type":"markdown","metadata":{"id":"9oj4PwyrXyJv"},"source":["# Load our image dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9Efl9GzfI3FW","executionInfo":{"status":"ok","timestamp":1763033548000,"user_tz":-120,"elapsed":458,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["(train_images_all, train_labels_all), (test_images_all, test_labels_all) = dataset\n","\n","# Get unique labels in our training dataset\n","unique_class_labels = np.unique(train_labels_all)\n","\n","# Normalize pixel values in the trainning and the test datasets to be between 0 and 1\n","# TODO: parameter of normalization minmax scalar\n","train_images_all, test_images_all = train_images_all / 255.0, test_images_all / 255.0\n","\n","if len(train_images_all.shape) == 3:\n","  # Make sure images have shape (28, 28, 1)\n","  train_images_all = np.expand_dims(train_images_all, -1)\n","  test_images_all = np.expand_dims(test_images_all, -1)\n","  print(\"x_train shape:\", train_images_all.shape)\n","  print(train_images_all.shape[0], \"train samples\")\n","  print(test_images_all.shape[0], \"test samples\")"]},{"cell_type":"markdown","metadata":{"id":"gNfz6W2ohXry"},"source":["# Synopsis block of code"]},{"cell_type":"markdown","metadata":{"id":"0_jB7PJGgjIj"},"source":["Optional: Check that our sampling method works appropriately"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4qBdBr-pI4du","executionInfo":{"status":"ok","timestamp":1763033548008,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["# A function that prints the occurence of each class in a list\n","def print_times_per_label(lst, labels_all):\n","  # Get unique labels in our training dataset\n","  unique_labels = np.unique(labels_all)\n","  for i in range(0, len(unique_labels)):\n","    print(\"Class\", unique_labels[i], \"has\", lst.count(i), \"samples in our dataset...\")"]},{"cell_type":"markdown","metadata":{"id":"DR08tk3jyWSm"},"source":["**Reservoir Sampling Function**:\n","*Randomized algorithms for randomly choosing k samples from a list of n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn’t fit into main memory.*\n","\n","Algorithm implemented:\n","\n","\n","1.   Create an array reservoir[0,...,k-1] and copy first k items of stream[ ] to it.\n","2.   Now one by one consider all items from (k+1)th item to nth item.\n","\n","\n","> Steps\n","\n","*   Generate a random number, denoted as j, from 0 to i where i is the index of the current item in stream[].\n","*   If j is in range 0 to k-1, replace reservoir[j] with stream[i]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b9X3b4IcI_TZ","executionInfo":{"status":"ok","timestamp":1763033548015,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["# Select k items from a stream of items-data\n","import random\n","\n","# A function to randomly select k items from stream[0..n-1].\n","def reservoir_sampling(stream, n, k):\n","  i = 0     # index for elements in stream[]\n","\n","  # reservoir[] is the output array.\n","  # Initialize it with first k elements from stream[]\n","  reservoir = [0] * k\n","\n","  for i in range(k):\n","    reservoir[i] = stream[i]\n","\n","  # Iterate from the (k+1)th element to Nth element\n","  while(i < n):\n","    # Pick a random index from 0 to i.\n","    j = random.randrange(i+1)\n","\n","    # If the randomly picked\n","    # index is smaller than k,\n","    # then replace the element\n","    # present at the index\n","    # with new element from stream\n","    if(j < k):\n","      reservoir[j] = stream[i]\n","    i+=1\n","\n","  return reservoir"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"HwQcQAJmJCxR","executionInfo":{"status":"ok","timestamp":1763033548022,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["# A function that finds the size of each reservoir for every class depending on its occurence in the initial dataset\n","# and returns the unique labels that exist in our dataset along with the corresponding percentage\n","def reservoir_size_per_class(init_labels):\n","\n","  # Get unique labels and their counts (how many times they appear) in our training dataset\n","  unique_labels, counts = np.unique(init_labels, return_counts = True)\n","\n","  # Transform to list\n","  unique_labels_lst = unique_labels.tolist()\n","  counts_lst = counts.tolist()\n","\n","  perc_per_class = []\n","  for i in range(len(unique_labels_lst)):\n","    perc_per_class.append(counts_lst[i]/len(init_labels))\n","\n","  # print(perc_per_class)\n","\n","  return perc_per_class, unique_labels_lst"]},{"cell_type":"markdown","metadata":{"id":"w8nxfspyhG88"},"source":["\"Pre-Processing Part (or Filtering)\": Call the sampling mehod. Get samples from the training and the testing datasets."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nUHVHwnXJQQH","executionInfo":{"status":"ok","timestamp":1763033548039,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["def sampling_method(sampling_method_id, train_images_all, train_labels_all, sample_size, test_images_all, test_labels_all, perc_test):\n","  print(\"Percentage of filtering in our training dataset was set:\")\n","  print(sample_size)\n","  if sampling_method_id == 0:\n","    # Simple reservoir sampling over the whole training dataset\n","    # Total size of the stream (or training dataset)\n","    n_train = len(train_images_all)\n","\n","    # Number of samples that will be drawn\n","    k_train = int(n_train * sample_size)\n","\n","    # Use the indexes of dataset in order to decide which samples will be drawn\n","    idx_tmp_train_list = list(range(0, n_train))\n","\n","    # Find the indexes in order to construct the dataset that will be used during the training process\n","    idx_train = reservoir_sampling(idx_tmp_train_list, n_train, k_train)\n","  else:\n","    # Reservoir sampling in each class based on the number of samples (per class) that exist in the initial dataset\n","    # Find the size of each reservoir for every class depending on its occurence in the initial training dataset\n","    class_perc, unique_ids = reservoir_size_per_class(train_labels_all)\n","\n","    # Stores the indexes (from all classes) in order to construct the dataset that will be used during the training process\n","    idx_train = []\n","\n","    # Run for every single class the reservoir sampling seperately\n","    for i in range(0, len(unique_ids)):\n","      # Find the locations of each sample belonging to our class of interest\n","      tmp = np.where(train_labels_all == unique_ids[i])\n","      idx_of_class = tmp[0].tolist()\n","\n","      # Run the reservoir sampling for the class of interest\n","      sampled_idx_of_class = reservoir_sampling(idx_of_class, len(idx_of_class), int(len(train_images_all) * sample_size * class_perc[i]))\n","\n","      # Store the (sampled) samples from this class\n","      for j in range(0, len(sampled_idx_of_class)):\n","        idx_train.append(sampled_idx_of_class[j])\n","\n","  # Store the corresponding images and labels from training dataset based on the sampled indexes\n","  train_images_lst = []\n","  for i in idx_train:\n","    train_images_lst.append(train_images_all[i])\n","\n","  train_labels_lst = []\n","  for i in idx_train:\n","    train_labels_lst.append(train_labels_all[i])\n","\n","  # Check the occurence of each class in the final training dataset\n","  print_times_per_label(train_labels_lst, train_labels_all)\n","\n","  # Total size of the stream (or testing dataset)\n","  n_test = len(test_images_all)\n","\n","  # Number of samples that will be drawn\n","  k_test = int(n_test * perc_test)\n","\n","  # Use the indexes of dataset in order to decide which samples will be drawn\n","  idx_tmp_test_list = list(range(0, n_test))\n","\n","  # Find the indexes in order to construct the dataset that will be used during the testing process\n","  idx_test = reservoir_sampling(idx_tmp_test_list, n_test, k_test)\n","\n","  # Store the corresponding images and labels from testing dataset based on the sampled indexes\n","  test_images_lst = []\n","  for i in idx_test:\n","    test_images_lst.append(test_images_all[i])\n","\n","  test_labels_lst = []\n","  for i in idx_test:\n","    test_labels_lst.append(test_labels_all[i])\n","\n","  # Tranfsorm the lists that we stored our samples into arrays\n","  train_images = np.asarray(train_images_lst)\n","  train_labels = np.asarray(train_labels_lst)\n","  test_images = np.asarray(test_images_lst)\n","  test_labels = np.asarray(test_labels_lst)\n","\n","  # Verify that the desired filtering was performed in both datasets\n","  print(\"Training dataset before sampling:\")\n","  print(train_images_all.shape)\n","  print(train_labels_all.shape)\n","  print(\"Training dataset after sampling:\")\n","  print(train_images.shape)\n","  print(train_labels.shape)\n","\n","  print(\"Testing dataset before sampling:\")\n","  print(test_images_all.shape)\n","  print(test_labels_all.shape)\n","  print(\"Testing dataset after sampling:\")\n","  print(test_images.shape)\n","  print(test_labels.shape)\n","\n","  return train_images, train_labels, test_images, test_labels"]},{"cell_type":"markdown","metadata":{"id":"UysAjOAyhmTa"},"source":["# CNN's architecture builder"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"IkUTMwRar3AR","executionInfo":{"status":"ok","timestamp":1763033548155,"user_tz":-120,"elapsed":112,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["def recreate_model(layers_lst, dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND):\n","  # Initialize a sequential model\n","  model = models.Sequential()\n","\n","  # Define the number of neurons for conv and dense layers\n","  conv_tmp2 = CONV_NEURONS_CONST\n","  dense_tmp2 = DENSE_NEURONS_CONST\n","\n","  # Recreate the model\n","  for count, layer in enumerate(layers_lst):\n","    # First layer has to be a convolutional one\n","    if layer == 'conv' and count == 0:\n","      model.add(layers.Conv2D(int(conv_tmp2), (3, 3), activation='relu', input_shape = dataset_shape))\n","      conv_tmp2 = conv_tmp2 * 2\n","    # For the other layers\n","    else:\n","      if layer == 'conv':\n","        # Add a conv layer by doubling its neurons if they do not violate our user-defined bound\n","        if conv_tmp2 <= CONV_NEURONS_BOUND:\n","          model.add(layers.Conv2D(int(conv_tmp2), (3, 3), activation='relu'))\n","          conv_tmp2 = conv_tmp2 * 2\n","        else:\n","          model.add(layers.Conv2D(int(CONV_NEURONS_BOUND), (3, 3), activation='relu'))\n","          conv_tmp2 = CONV_NEURONS_BOUND\n","      elif layer == 'pool':\n","        # Add a pool layer\n","        model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n","      else:\n","        # If the next to-be-added-layer is dense and no other dense layer has been added so far, then add a flatten layer first...\n","        if dense_tmp2 == DENSE_NEURONS_CONST:\n","          model.add(layers.Flatten())\n","        # Add a dense layer by reducing (* 0.5) its neurons if they do not violate our user-defined bound\n","        if dense_tmp2 >= DENSE_NEURONS_BOUND:\n","          model.add(layers.Dense(int(dense_tmp2), activation='relu'))\n","          dense_tmp2 = dense_tmp2 / 2\n","        else:\n","          model.add(layers.Dense(int(DENSE_NEURONS_BOUND), activation='relu'))\n","          dense_tmp2 = DENSE_NEURONS_BOUND\n","\n","  return model, conv_tmp2, dense_tmp2"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2RxKOsFbEDQF","executionInfo":{"status":"ok","timestamp":1763033548162,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["def my_evaluate_first_phase(q, train_images, train_labels, test_images, test_labels, epochs, lr, size_of_batch):\n","  # Initialize a sequential network\n","  model = models.Sequential()\n","\n","  # Define the neurons of conv and dense layers based on user's input\n","  conv_tmp = CONV_NEURONS_CONST\n","  dense_tmp = DENSE_NEURONS_CONST\n","\n","  # Add the (first) conv layer\n","  model.add(layers.Conv2D(int(CONV_NEURONS_CONST), (3, 3), activation='relu', input_shape = dataset_shape))\n","  conv_tmp = conv_tmp * 2\n","\n","  # Add manually a flatten and a dense layer in order to evaluate the network\n","  model.add(layers.Flatten())\n","  model.add(layers.Dense(len(unique_class_labels), activation='softmax'))\n","\n","  model.compile(optimizer=Adam(lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","  model.summary()\n","\n","  start = time.time()\n","\n","  blackbox = model.fit(x=train_images,\n","                      y=train_labels,\n","                      epochs=epochs,\n","                      batch_size=size_of_batch\n","                      )\n","  stop = time.time()\n","\n","  # Compute the training speed of this CNN architecture\n","  tr_time = stop - start\n","\n","  # Compute the accuracy of our training model in the testing dataset\n","  test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n","\n","  # Compute the metric that captures the accuracy--speed tradeoff\n","  # tradeOff_metric = lamda_acc * test_acc - (1 - lamda_acc) * math.tanh(tr_time/theta_parameter - 1)\n","\n","  # inverse additive penalty\n","  # tradeOff_metric = test_acc / (1 + 0.5 * min(1, tr_time/theta_parameter))\n","\n","  # log-sigmoid dominance\n","  tradeOff_metric = 1 / (1 + np.exp(-0.5 * (test_acc - (1 - min(1, tr_time/theta_parameter)))))\n","\n","  del model\n","\n","  print(\"========================== EDW EINAI TO MULTI1111111111111111.......\")\n","  print(\"Accuracy (on the testing dataset): {0:.2%}\".format(test_acc))\n","  print(f\"Training time: \", tr_time)\n","  print(tradeOff_metric)\n","\n","  q.put([test_acc, tr_time, tradeOff_metric])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"6lyi_kn3ErWi","executionInfo":{"status":"ok","timestamp":1763033548170,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["def my_evaluate_rest_phase(q, train_images, train_labels, test_images, test_labels, layer, current_df, dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND):\n","  error_flag = -1\n","\n","  # Recreate the network that consist of the best layers that we found in each of the previous steps/iterations\n","  model, conv_tmp, dense_tmp = recreate_model(current_df['LayerType'], dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND)\n","\n","  try:\n","    # If the to-be-added-layer is conv\n","    if layer == 'conv':\n","      # Add a conv layer by doubling its neurons if they do not violate our user-defined bound\n","      if conv_tmp <= CONV_NEURONS_BOUND:\n","        model.add(layers.Conv2D(int(conv_tmp), (3, 3), activation='relu'))\n","        conv_tmp = conv_tmp * 2\n","      else:\n","        model.add(layers.Conv2D(int(CONV_NEURONS_BOUND), (3, 3), activation='relu'))\n","    # If the to-be-added-layer is pool\n","    elif layer == 'pool':\n","      model.add(layers.MaxPooling2D((2, 2), strides=(2,2), padding='same'))\n","    # If the to-be-added-layer is dense\n","    else:\n","      # If the next to-be-added-layer is dense and no other dense layer has been added so far, then add a flatten layer first...\n","      if not isinstance(model.layers[-1], tf.keras.layers.Dense):\n","        model.add(layers.Flatten())\n","      # Add a dense layer by reducing (* 0.5) its neurons if they do not violate our user-defined bound\n","      if dense_tmp >= DENSE_NEURONS_BOUND:\n","        model.add(layers.Dense(int(dense_tmp), activation='relu'))\n","        dense_tmp = dense_tmp / 2\n","      else:\n","        model.add(layers.Dense(int(DENSE_NEURONS_BOUND), activation='relu'))\n","\n","\n","    # Check if the last layer is a Dense layer\n","    last_layer = model.layers[-1]\n","\n","    # Check if the last layer of the network is dense\n","    # If it is just add the last dense layer for the classification\n","    # Otherwise you should first add a flatten layer\n","    if isinstance(last_layer, tf.keras.layers.Dense):\n","        model.add(layers.Dense(len(unique_class_labels), activation='softmax'))\n","    else:\n","        model.add(layers.Flatten())\n","        model.add(layers.Dense(len(unique_class_labels), activation='softmax'))\n","\n","  except ValueError:\n","        print(\"No valid input...:(\")\n","        error_flag = 1\n","\n","  if error_flag == -1:\n","    model.compile(optimizer=Adam(current_df['LearningRate']), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","    model.summary()\n","\n","    start = time.time()\n","\n","    blackbox = model.fit(x=train_images,\n","                        y=train_labels,\n","                        epochs=current_df['Epochs'],\n","                        batch_size=current_df['BatchSize']\n","                        )\n","    stop = time.time()\n","\n","    # Compute the training speed of this CNN architecture\n","    tr_time = stop - start\n","\n","    # Compute the accuracy of our training model in the testing dataset\n","    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n","\n","    # Compute the metric that captures the accuracy--speed tradeoff\n","    # tradeOff_metric = lamda_acc * test_acc - (1 - lamda_acc) * math.tanh(tr_time/theta_parameter - 1)\n","\n","    # inverse additive penalty\n","    # tradeOff_metric = test_acc / (1 + 0.5 * min(1, tr_time/theta_parameter))\n","\n","    # log-sigmoid dominance\n","    tradeOff_metric = 1 / (1 + np.exp(-0.5 * (test_acc - (1 - min(1, tr_time/theta_parameter)))))\n","\n","    # Delete the Keras model with these hyper-parameters from memory.\n","    del model\n","\n","    print(\"========================== EDW EINAI TO MULTI2222222222.......\")\n","    print(\"Accuracy (on the testing dataset): {0:.2%}\".format(test_acc))\n","    print(f\"Training time: \", tr_time)\n","    print(tradeOff_metric)\n","\n","    q.put([test_acc, tr_time, tradeOff_metric])\n","  else:\n","    q.put([0, 1000000000, 0])"]},{"cell_type":"code","source":["def my_evaluate(q, train_images, train_labels, test_images, test_labels, lst_layers, dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND, my_epochs, my_lr, my_batch):\n","\n","  error_flag = -1\n","\n","  if lst_layers[0] == 'conv':\n","\n","    try:\n","      # Recreate the network that consist of the best layers that we found in each of the previous steps/iterations\n","      model, conv_tmp, dense_tmp = recreate_model(lst_layers, dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND)\n","\n","      # Check if the last layer is a Dense layer\n","      last_layer = model.layers[-1]\n","\n","      # Check if the last layer of the network is dense\n","      # If it is just add the last dense layer for the classification\n","      # Otherwise you should first add a flatten layer\n","      if isinstance(last_layer, tf.keras.layers.Dense):\n","          model.add(layers.Dense(len(unique_class_labels), activation='softmax'))\n","      else:\n","          model.add(layers.Flatten())\n","          model.add(layers.Dense(len(unique_class_labels), activation='softmax'))\n","\n","    except ValueError:\n","          print(\"No valid input...:(\")\n","          error_flag = 1\n","\n","    if error_flag == -1:\n","      model.compile(optimizer=Adam(my_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","      model.summary()\n","\n","      start = time.time()\n","\n","      blackbox = model.fit(x=train_images,\n","                          y=train_labels,\n","                          epochs=my_epochs,\n","                          batch_size=my_batch\n","                          )\n","      stop = time.time()\n","\n","      # Compute the training speed of this CNN architecture\n","      tr_time = stop - start\n","\n","      # Compute the accuracy of our training model in the testing dataset\n","      test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n","\n","      # Compute the metric that captures the accuracy--speed tradeoff\n","      # tradeOff_metric = lamda_acc * test_acc - (1 - lamda_acc) * math.tanh(tr_time/theta_parameter - 1)\n","\n","      # inverse additive penalty\n","      # tradeOff_metric = test_acc / (1 + 0.5 * min(1, tr_time/theta_parameter))\n","\n","      # log-sigmoid dominance\n","      tradeOff_metric = 1 / (1 + np.exp(-0.5 * (test_acc - (1 - min(1, tr_time/theta_parameter)))))\n","\n","      # Delete the Keras model with these hyper-parameters from memory.\n","      del model\n","\n","      print(\"========================== EDW EINAI TO MULTI2222222222.......\")\n","      print(\"Accuracy (on the testing dataset): {0:.2%}\".format(test_acc))\n","      print(f\"Training time: \", tr_time)\n","      print(tradeOff_metric)\n","\n","      q.put([test_acc, tr_time, tradeOff_metric])\n","    else:\n","      q.put([0, 1000000000, 0])\n","  else:\n","    q.put([0, 1000000000, 0])"],"metadata":{"id":"2QLXwoLD-Prj","executionInfo":{"status":"ok","timestamp":1763033548184,"user_tz":-120,"elapsed":10,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import itertools\n","\n","combinations = list(itertools.product(set_of_layers, repeat= total_num_of_layers))"],"metadata":{"id":"3JHIHOHi7xRZ","executionInfo":{"status":"ok","timestamp":1763033548200,"user_tz":-120,"elapsed":6,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1SFFcvd3phBCKC86TrQnxsSEwMjVoNIFN"},"id":"neeLDKAjOkEQ","outputId":"ceea6ede-d134-4382-dd44-913f6cffd425","executionInfo":{"status":"ok","timestamp":1763040257362,"user_tz":-120,"elapsed":6709159,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["start_program = time.time()\n","\n","# Get the shape of the input dataset\n","dataset_shape = train_images_all.shape[1:]\n","\n","# Store the best type of layer of each step\n","best_score = -1000\n","best_sampling = -1\n","best_epochs = -1\n","best_layers = -1\n","best_acc = -1\n","best_train = -1\n","\n","# The first layer in this set of datasets has to be a convolutional one!\n","# So our \"search space\" is all combinations of epochs and sampling rates based on user's input\n","for sample_size in np.arange(sample_size_low, sample_size_high + 0.01, sample_size_step):\n","  for epochs in range(num_of_epochs_low, num_of_epochs_high + 1, num_of_epochs_step):\n","    for combo in combinations:\n","      for lr in lr_list:\n","        for size_of_batch in range(size_of_batch_low, size_of_batch_high + 1, size_of_batch_step):\n","\n","          print(combo)\n","\n","          # Perform the sampling\n","          train_images, train_labels, test_images, test_labels = sampling_method(sampling_method_id, train_images_all, train_labels_all, sample_size, test_images_all, test_labels_all, perc_test)\n","          q = Queue()\n","          process_eval = multiprocessing.Process(target=my_evaluate, args=(q, train_images, train_labels, test_images, test_labels, combo, dataset_shape, CONV_NEURONS_CONST, DENSE_NEURONS_CONST, CONV_NEURONS_BOUND, DENSE_NEURONS_BOUND, epochs, lr, size_of_batch))\n","          process_eval.start()\n","          test_acc, tr_time, tradeOff_metric = q.get()\n","          process_eval.join()\n","\n","          # Print the results.\n","          print()\n","          print(\"Accuracy (on the testing dataset): {0:.2%}\".format(test_acc))\n","          print(f\"Training time: \", tr_time)\n","          print(tradeOff_metric)\n","          print()\n","\n","          # Delete the dfs.\n","          del train_images\n","          del train_labels\n","          del test_images\n","          del test_labels\n","\n","          # Clear the Keras session, otherwise it will keep adding new\n","          # models to the same TensorFlow graph each time we create\n","          # a model with a different set of hyper-parameters.\n","          K.clear_session()\n","          tf.compat.v1.reset_default_graph()\n","\n","          if tradeOff_metric > best_score:\n","            best_score = tradeOff_metric\n","            best_sampling = sample_size\n","            best_epochs = epochs\n","            best_layers = combo\n","            best_acc = test_acc\n","            best_train = tr_time\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"EoFxBX9pGOCr","executionInfo":{"status":"ok","timestamp":1763040257409,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}}},"outputs":[],"source":["stop_program = time.time()"]},{"cell_type":"markdown","metadata":{"id":"LDbXnCD1ZEXW"},"source":["# Results"]},{"cell_type":"code","source":["print(best_score)\n","print(best_sampling)\n","print(best_epochs)\n","print(best_layers)\n","print(best_acc)\n","print(best_train)"],"metadata":{"id":"Mcc0PsFFEz96","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763040257418,"user_tz":-120,"elapsed":6,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}},"outputId":"7d30d85a-144a-4db9-e44b-8ce110c875f8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5929200577106841\n","0.9500000000000001\n","5\n","('conv', 'conv', 'pool', 'conv', 'conv')\n","0.7520999908447266\n","23.256213426589966\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"rrko8cHzGSPg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763040257429,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nikos Giatrakos Athena Research Center","userId":"17896459333690309024"}},"outputId":"2387a4e1-4b26-43b0-8500-006c1c828a93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6709.429879188538"]},"metadata":{},"execution_count":16}],"source":["# Compute the time\n","program_time = stop_program - start_program\n","program_time"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}