In machine learning applications over Big streaming Data, Neural Networks (NNs) are continuously trained on large volumes of rapidly ingested streams. Online model update enables NNs to maintain high performance on the desired predictive tasks (e.g., classification) in the long run, by continuously adapting the neural model to volatile statistical properties and distributions of the ever-changing ingested training streams. As soon as a new version of the NN becomes available, it immediately gets deployed for inference purposes. However, the real-time character of relevant autonomous business processes lying in the intersection of Big streaming Data and real-time, online Neural Learning creates unique challenges for systems since the trade-off between training speed and predictive accuracy largely depends on the complexity of the NN and the rate at which new data arrives. To address this, we recently demonstrated SuBiTO, a framework that automatically and continuously monitors the training time vs accuracy trade-offs as Big streaming Data arrive and fine-tunes: (i) the number, size, and type of NN layers; (ii) the size of the ingested data via stream synopses; and (iii) the number of training epochs. In this work, we unveil the algorithmic foundations behind the novel synopsis-based training optimization paradigm introduced by SuBiTO. We present a suite of optimization algorithms that can be used for adaptive and continuous optimization of the neural training pipeline so as to achieve good training time vs accuracy trade-offs. The algorithmic suite we contribute includes Exhaustive, Greedy, Bayesian Optimization-based, Heuristic, and progressive Evolutionary search algorithms which cover a variety of demands for different application scenarios. Experimental results over image and video streams demonstrate that the SuBiTO algorithmic suite offers complementary characteristics on the suggested training pipeline configurations allowing applications and users to select the most appropriate algorithm based on application constraints and performance goals. In that, we provide evidence on the flexibility and suitability of SuBiTO under diverse streaming training settings.
